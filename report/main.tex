\documentclass{article}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{overpic}
\usepackage{amssymb}

\usepackage[accepted]{dlai2022}

%%% STUDENTS: FILL IN WITH YOUR OWN INFORMATION
\dlaititlerunning{DLAI Project: World Models in MiniGrid}

\begin{document}

\twocolumn[
%%% STUDENTS: FILL IN WITH YOUR OWN INFORMATION
\dlaititle{DLAI Project: World Models in MiniGrid}

\begin{center}\today\end{center}

\begin{dlaiauthorlist}
%%% STUDENTS: FILL IN WITH YOUR OWN INFORMATION
\dlaiauthor{Dennis Rotondi}{}
% \dlaiauthor{Donato Crisostomi}{}
% \dlaiauthor{Emanuele Rodol√†}{}
\end{dlaiauthorlist}

%%% STUDENTS: FILL IN WITH YOUR OWN INFORMATION
\dlaicorrespondingauthor{Dennis Rotondi}{rotondi.1834864@studenti.uniroma1.it}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}
%
In this report are presented the reasoning and investigations of Dennis Rotondi 1834864 to complete the project 06 of the course DLAI 2022.
\href{https://github.com/DennisRotondi/dlai_project}{Code available here.}

\end{abstract}

\section{Introduction}
Sequential decision making, commonly formalized as Markov Decision Process (MDP) optimization, is a key challenge in artificial intelligence. Two successful approaches to solve this problem are planning
and reinforcement learning. These may actually
be combined, in a field which is known as model-based reinforcement learning; for which one definition is: "any MDP approach that $i)$ uses a model (known or learned) and $ii)$
uses learning to approximate a global value or policy function" \cite{mb_survey}.
It's important to notice that the only engagement of a deep learning architecture does not make an algorithm model based, indeed in 
traditional RL (model-free RL) small networks are used to improve the learned policy through trial and error, but due to the bottleneck of credit assignment problem, i.e. figuring out which steps caused the resulting feedback or which should be blamed for the final result, it's hard to handle
NNs with million of parameters as predictive structure. Instead in this relatively new paradigm it's all about the anology that to handle the vast amount of information that flows through our daily lives, our brain learns an abstract representation of both spatial and temporal aspects of this information \cite{brain_article}, thus large structures are not only possible but also encouraged:  World models are an explicit
way to represent an agent's knowledge about its environment in a parametric model that can make predictions about the future.
\section{Related Work}
Model Based RL is not an invention of the new millennium, many 
research papers yet in '90s were discussing and analyzing the different RL branches \cite{Atkeson97acomparison}.
 However it's with the recent explosion of deep learning  also thanks to the 
 outstanding frameworks developed and modern GPU accelerators of the last years 
 that new progresses have been achieved, a line of the new era can be delimited by the nice World Model work 
 \cite{wm}: in what follow we'll focus on this generation of algorithms. 
 To solve the Car Racing and VizDoom scenario, here the authors built three neural networks: one to learn the environment, one to memorize and predict the next observation and a final one to control the agent. 
 On this skeleton google research realized and published different models to master the Atari games: SimPLe \cite{simple}, where
 a policy is deployed to collect more data in the original game, resulting is significantly more sample-efficient than a highly
 tuned version of the state of the art algorithms; DreamerV2 \cite{dreamerv2} builds upon the Recurrent State-Space Model also used in PlaNet \cite{planet} and DreamerV1 \cite{dreamerv1}, its encoder embed each visual observation into a 32 distributions over 32 classes each, the meanings of which are determined automatically as the world model learns: it achieves human-level performance! 
\section{Method}
For the pythonic part of the project I've decided to reproduce the structure of the reference paper suggested~\cite{wm} applied to the 
the Minigrid gym \cite{gym_minigrid}, a very simple grid-based environment which has the pro of having many variants to test if the learned policy is something that
the agent has really developed inside the world or not.
\section{Results}
\section{Discussion and conclusions}
Recent advances in deep RL have enabled model-based approaches 
to learn accurate world models from image inputs, this can be a shortcut to
produce faster good policies since world models facilitate generalization and can predict the outcomes of 
potential actions to enable planning. Learning with statistical uncertainty in the virtual environment make the final policy more robust
as shown in~\cite{wm} for the VizDoom game, there the score attained in the real environment was much higher than the one obtained inside the dream.
But being in a dream has not only positive sides: inevitably we introduce sources of approximation error
that could be converted into quibble that allows the agent to cheat: for example some position where enemies cannot hit him or
byway in the labyrinth not intended to be there: this implies to take into account a problem not present in the original environment.
Possible evolutions of this paradigm are the unification of the structure that we've seen dividend in 3 pieces, in such a way that even running an action at the end of the pipeline
take a role in the learning process of the previous parts. Moreover possible research directions could be
to replace VAE and MDN-RNN with higher capacity models like Transformers \cite{transformers}. Extending the latent space as done by \cite{dreamerv2} is the key to reduce the
object vanishing problem, furthermore google brain sees world models that leverage large offline datasets, long-term memory, hierarchical planning, and directed exploration.\\
Completing this homework has been a full journey into the deep learning field: 
I started from making the dataset, passing through the creation and training 
of three different neural networks using the standard approach and a more 
complex one required by the non-convexity of the problem; this while exploiting 
many of the most recent and sophisticated tools in the life of a machine learning scientist.

% ------------------------------------------------------------------------------
\section{Using \LaTeX}\label{sec:latex}

If this is your first time using \LaTeX, here are a few common instructions that you may find useful. Please read this while looking at the source code.

\paragraph*{Formulas.}
You can write formulas inline, such as $x^2$, or you can put them in their own environment, for example:
%
\begin{equation}\label{eq:dirichlet}
\lambda_i = \int_\mathcal{X} \langle \nabla \phi_i(x), \nabla \phi_i(x) \rangle dx \,.
\end{equation}

You can also refer to formulas without having to write their equation number by hand, such as Equation~\eqref{eq:dirichlet}.

\paragraph*{Figures.}
You can and are encouraged to include figures. See an example with Figure~\ref{fig:torus}.

\begin{figure}[t]
    \centering
    \begin{overpic}[width=0.99\linewidth]{./imgs/torus.png}
    \put(-1, 21){\color{blue}\footnotesize $\mathcal{M}_2$ }
    \put(13, 12){\color{red}\footnotesize $\mathcal{M}_1$ }
    \put(93, 30){\footnotesize $\mathcal{Z}$ }
    \put(79, 26){\scriptsize $z_2$ }
    \put(88, 26){\scriptsize $z_1$ }
    \end{overpic}
    \caption{In the figure caption, you can write what you want including formulas, e.g. $\mathcal{X} \subset \mathbb{R}^3$. Notice that in this figure, we added mathematical symbols on top of the image by using the overpic command.}
    \label{fig:torus}
\end{figure}

\paragraph*{Table.}
Tables can be used to report quantitative results, here is one random example:

\begin{table}[h!]
\caption{Performance comparison.}
\label{tab:results}
\begin{center}
\begin{small}
\begin{tabular}{p{0.16\linewidth} | ccccc}
\toprule
& \multirow{2}{0.1\linewidth}{$\beta$ VAE}& \multirow{2}{0.1\linewidth}{DCI Dis.}& \multirow{2}{0.1\linewidth}{MIG}& \multirow{2}{0.1\linewidth}{MIG-PCA}& \multirow{2}{0.1\linewidth}{MIG-KM}\\
\#factors \\
\midrule
One      & 100\% & \textbf{99.0\%} &  63.7\% & 73.5\% &  69.2\%  \\
Variable & 98.9\% & 94.9\% & 62.3\% &  70.5\%& \textbf{66.9\%} \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vspace{-0.5cm}
\end{table}

\newpage
~\newpage
\bibliography{references.bib}
\bibliographystyle{dlai2022}

\end{document}